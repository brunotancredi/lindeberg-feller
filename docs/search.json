[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "",
    "text": "Here, I’m going to present generalizations of the Central Limit Theorem, such as the Lindeberg–Levy–Feller Central Limit Theorem and Lyapunov’s Central Limit Theorem. The applications of the asymptotic behavior of the sample mean of random variables extend across all areas of statistics, being used for inference, hypothesis testing, estimation theory, among many others.\nThe reference material has been drawn from the books Probability: A Graduate Course (Gut, 2013) and Asymptotic Statistics (Vaart, 1998), from the Large Sample Theory lectures by Prof. Jessica Li available on YouTube (Li, 2020), and from the lecture notes by Prof. Marco Scavino for the Probabilidad II course at the Universidad de la República."
  },
  {
    "objectID": "index.html#central-limit-theorem",
    "href": "index.html#central-limit-theorem",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nProbably we are familiar with the Central Limit Theorem (Theorem 1).\n\nTheorem 1 (Central Limit Theorem) Let \\(\\{X_n\\}_{n \\geq  1}\\) be a sequence of independent and identical distributed random variables with finite expectation \\(\\mu\\) and and positive, finite variance \\(\\sigma^2\\), and set \\(S_n = X_1 + X_2 + \\dots + X_n, n \\geq 1\\). Then\n\\[\n\\frac{S_n - n\\mu}{\\sigma\\sqrt{n}} \\, \\overset{d}{\\rightarrow} \\, \\mathcal{N}(0,1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{1}\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nProbably you have seen the CLT stated in terms of the sample mean \\(\\bar{X}_n = \\frac{S_n}{n}\\) as follows:\n\\[\n\\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma} \\, \\overset{d}{\\rightarrow} \\, \\mathcal{N}(0,1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\]\nBut is easy to see that is analagous at the one stated above\n\\[\n\\sqrt{n}\\frac{\\bar{X}_n - \\mu}{\\sigma}\n= \\sqrt{n}\\frac{\\frac{S_n}{n} - \\mu}{\\sigma}\n= \\sqrt{n}\\frac{S_n - n\\mu}{\\sigma n}\n= \\frac{S_n - n\\mu}{\\sigma \\sqrt{n}}\n\\]\n\n\n\n\nSimulating the CLT\nThe widget below shows the Central Limit Theorem in action. It uses 5,000 Monte Carlo simulations for different sample sizes under uniform and exponential distributions, and displays the resulting histogram of the sample mean.\n\nimport { aq, op } from '@uwdata/arquero'\n\nmy_data = aq.loadCSV(\"https://raw.githubusercontent.com/brunotancredi/lindeberg-feller/refs/heads/main/CLT_simulation_data.csv\")\n\nviewof n = Inputs.range(\n  [1, 200], \n  {value: 1, step: 1, label: \"Sample Size:\"}\n)\n\nviewof distribution = Inputs.radio([\"Uniform\", \"Exponential\"], {value: \"Uniform\", label: \"Distribution\"})\n\nnormalData = Array.from({ length: 100 }, (_, i) =&gt; {\n  let x = -3 + (i / 99) * 6; // Range from -3 to 3\n  return { x, y: Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI) };\n});\n\n\nmy_data_2 = my_data.params({ distribution }).filter(d =&gt; d.distribution === distribution);\n\npoint = Array.from({length: 1}, (_, i) =&gt; {return {x: 0, y: 0.5}})\n\nPlot.plot({\n    width: 1000,\n    height: 500,\n    y: {grid: true},\n    x: {round: true},    \n    marks: [\n      Plot.rectY(my_data_2, \n                Plot.binX({y: (a, bin) =&gt; {\n                              return a.length / (5000*(bin.x2 - bin.x1));\n                           }}, \n                          {x: {value: n.toString(), \n                               inset: 0,\n                               thresholds: d3.range(-3,3 + 0.11, 0.11),\n                               domain: [-3, 3],\n                               }})),\n      Plot.ruleY([0]),\n      Plot.line(normalData, { x: \"x\", y: \"y\", stroke: \"red\" }), //Normal\n      Plot.dot(point, {x: \"x\", y: \"y\", r: 0})    \n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe problem with the theorem is that requires a two strong assumptions independency and identical distribution, what if I told you that we can relax one of the conditions?"
  },
  {
    "objectID": "index.html#lindeberg-lévy-feller",
    "href": "index.html#lindeberg-lévy-feller",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Lindeberg-Lévy-Feller",
    "text": "Lindeberg-Lévy-Feller\nLet \\(\\{X_n\\}_{n \\geq 1}\\) be a sequence of independent random variables with finite variances, and set, for \\(k \\geq 1, \\mathbb{E}[X_k]= \\mu_k, Var[X_k] = \\sigma_k^2\\) and for \\(n \\geq 1, S_n = \\sum^{n}_{k=1}X_k\\) and \\(V_n^2 = \\sum^n_{k=1}\\sigma_k^2\\)\n\nConditions\n\\[\nL_1(n) = \\frac{1}{V_n^2}\\sum^{n}_{k=1}\\mathbb{E}\\big[(X_k - \\mu_k)^2\\mathbf{1}_{\\{|X_k - \\mu_k| \\gt \\epsilon V_n\\}}\\big] \\underset{n \\rightarrow \\infty}{\\rightarrow} 0, \\quad \\forall \\epsilon \\gt 0\n\\tag{2}\\]\n\\[\nL_2(n) = \\max_{1 \\leq k \\leq n} \\frac{\\sigma^2_k}{V_n} \\underset{n \\rightarrow \\infty}{\\rightarrow} 0,\n\\tag{3}\\]\n\n\nInterpretating the conditions\n\\(L_1\\) (Lindeberg’s Condition): This condition requires that the proportion of variance represented by values far from the mean is negligible. In other words, what it asks us is that the original distributions of \\(X_k\\) do not have heavy tails.\n\n\n\n\n\n\nFigure 1: Rudimentary visualization of Lindeberg Condition\n\n\n\n\\(L_2\\): To understand condition \\(L_2\\), it is useful to consider when it fails. The condition is violated when \\(\\sigma_k^2\\) represents a non-negligible portion of the total sum of variances. In other words, this occurs when the variance of a single term contributes significantly to the total variance. For example, this could happen when \\(\\sigma^2_k = \\sum_{i=1}^{k-1} \\sigma_i^2\\). So we are asking o each \\(X_k\\) to have a limited variance.\n\n\nTheorem\nUsing the conditions defined above we can state the theorem as follows\n\nTheorem 2 (Lindeberg-Lévy-Feller Theorem) \n(i). If \\(L_1\\) (Equation 2) is satisfied , then so is \\(L_2\\) (Equation 3) and\n\\[\n\\frac{1}{V_n}\\sum^{n}_{k=1}(X_k - \\mu_k) \\overset{d}{\\rightarrow} \\mathcal{N}(0, 1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{4}\\]\n(ii). If Equation 3 and Equation 4 are satisfied, then so is Equation 2\n\nAs we can see, we no longer require that the sequence be identically distributed to obtain a normal distribution. We are only requiring (i) that \\(L_1\\) is satisfied."
  },
  {
    "objectID": "index.html#what-do-we-need-to-prove-it",
    "href": "index.html#what-do-we-need-to-prove-it",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "What do we need to prove it?",
    "text": "What do we need to prove it?\nThe proof of the theorem is extensive, so here I’m going to present a sketch of the proof.\nFirst, for simplicity and without loss of generality, we assume that \\(\\mu_k = 0\\) for all \\(k \\geq 1\\).\nSo we want to prove that\n\\[\n\\frac{S_n}{V_n} \\overset{d}{\\rightarrow} Z,\n\\quad \\text{as} \\quad n \\rightarrow \\infty\n\\]\nwhere \\(Z \\sim \\mathcal{N}(0,1)\\).\n\nFor the proof we are going to use the Lévy’s Continuity Theorem (Theorem 3), which establishes a relationship between convergence in distribution and convergence of characteristic functions.\n\nTheorem 3 (Lévy’s Continuity Theorem) Let \\(\\{X_n\\}_{n \\geq 1}\\) be a sequence of random variables. Then\n\\[\n\\begin{aligned}\n&\\qquad \\varphi_{X_n}(t) \\rightarrow \\varphi_X(t)\n\\quad \\text{as } n \\rightarrow \\infty\n\\quad \\forall t \\in \\mathbb{R}, \\\\\n&\\Leftrightarrow \\quad\nX_n \\overset{d}{\\rightarrow} X\n\\quad \\text{as } n \\rightarrow \\infty.\n\\end{aligned}\n\\]\n\nBy proving that \\(\\varphi_{\\frac{S_n}{V_n}}(t) \\to \\varphi_Z(t)\\) we can conclude that \\(\\frac{S_n}{V_n} \\xrightarrow{d} Z\\) which is the result we want.\nTo show that \\(\\varphi_{\\frac{S_n}{V_n}}(t)\\) converges pointwise to the characteristic function of the standard normal distribution \\(\\varphi_Z(t)\\), we proceed as follows: We approximate \\(\\varphi_{\\frac{S_n}{V_n}}(t)\\) using its Taylor expansion. This approximation leads to an expression that coincides with \\(\\varphi_Z(t)\\), and we then show that the remainder (error) term tends to zero as \\(n \\to \\infty\\).\n\n\n\n\n\n\nSketch of the proof\n\n\n\n\n\n\\[\n\\varphi_{\\frac{S_n}{V_n}}(t) = \\varphi_{S_n}\\!\\left( \\frac{t}{V_n} \\right)\n\\]\n\\[\n\\left( S_n = \\sum_{k=1}^n X_k, \\quad X_k \\text{ indep.} \\right)\n\\]\n\\[\n= \\prod_{k=1}^n \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right)\n= \\left( \\varphi = \\exp(\\log \\varphi) \\right)\n\\]\n\\[\n= \\exp\\!\\left\\{ \\sum_{k=1}^n \\log \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right) \\right\\}\n\\]\n\\[\n\\overset{(*)}{\\approx}\n\\exp\\!\\left\\{ -\\sum_{k=1}^n \\left( 1 - \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right) \\right) \\right\\}\n\\]\n\\[\n\\overset{(**)}{\\approx}\n\\exp\\!\\left\\{ -\\sum_{k=1}^n\n\\left( 1 - \\left( 1 + \\frac{it}{V_n} \\mathbb{E}X_k + \\frac{(it)^2}{2V_n^2} \\mathbb{E}X_k^2 \\right) \\right)\n\\right\\}\n\\]\n\\[\n= \\exp\\!\\left\\{ -\\sum_{k=1}^n\n\\left( 1 - \\left( 1 - \\frac{t^2}{2V_n^2}\\sigma_k^2 \\right) \\right)\n\\right\\}\n\\]\n\\[\n= \\exp\\!\\left\\{ -\\frac{t^2}{2V_n^2} \\sum_{k=1}^n \\sigma_k^2 \\right\\}\n= e^{-\\frac{t^2}{2}}.\n\\]\nWhere \\((*)\\) follows from the Taylor expansion of \\(\\log(1 + x)\\) around \\(x = 0\\) and \\((**)\\) follows from the Taylor expansion of \\(\\varphi_{X_k}(t)\\) around \\(t = 0\\). We need to prove that those \\(\\approx\\) are \\(=\\) when \\(n \\to \\infty\\), i.e. the error terms vanish.\n\n\n\nThis combination of Lévy’s Continuity Theorem and Taylor series expansion can be used to prove different theorems, including the original Central Limit Theorem."
  },
  {
    "objectID": "index.html#example",
    "href": "index.html#example",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Example",
    "text": "Example\nA cool application of Lindeberg-Feller theorem is in the area of Linear Regression. For those that are in a Regression course, or already take one, you will have seen that the Ordinary Least Squares (OLS) estimator follows a normal distribution when the errors are normal. But … what if the errors are not normal? Well, under certain assumptions we can use the Lindeberg-Lévy-Feller theorem to show that the OLS estimator assympotically follows a normal distribution. For simplicity we are going to prove it for the estimator of the slope coefficient in a simple linear regression model (SLR).\n\\[\ny_i = \\beta_0 + \\beta_1x_i + e_i, \\quad e_i \\quad i.i.d \\quad \\forall i = 1, \\dots, n\n\\tag{5}\\]\nA simple linear regression model is given by Equation 5, where \\(\\mathbb{E}[e_i] = 0\\) and \\(\\text{Var}(e_i) = \\sigma^2\\). We have that the estimator for the slope in SLR is given by\n\\[\n\\hat{\\beta}_{1n} = \\frac{\\sum_{i=1}^n(y_i - \\bar{Y}_ n)(x_i - \\bar{X}_n)}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\tag{6}\\]\nYou can notice that in Equation 6 we have a subindex \\(n\\), this is because \\(\\hat{\\beta}_{1n}\\) (and the means \\(\\bar{X}_n\\), \\(\\bar{Y}_n\\) too) changes with the sample size, and we want to study its asymptotic behavior when \\(n \\to \\infty\\).\nAfter some math we can rewrite \\(\\hat{\\beta}_{1n}\\) as a linear combination of the erros\n\\[\n\\hat{\\beta}_{1n} = \\beta_1 + \\frac{\\sum_{i=1}^n(x_i - \\bar{X}_n)e_i}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\tag{7}\\]\nWe can define the sequence of random variables\n\\[\nW_{ni} := (x_i - \\bar{X}_n)e_i\n\\]\nAnd rewrite Equation 7 as\n\\[\n\\hat{\\beta}_{1n} = \\beta_1 + \\frac{\\sum_{i=1}^nW_{ni}}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\]\nWe have that the expected value of \\(W_{ni}\\) is zero and its variance is given by \\(\\sigma^2(x_i - \\bar{X})^2\\), since the only random part is \\(e_i\\) and they are i.i.d, with mean zero and variance \\(\\sigma^2\\).\nOur goal is to proof that \\[\n\\frac{\\sum_{i=1}^{n}W_{ni}}{\\sigma\\sqrt(\\sum_{i=1}^{n}(x_i - \\bar{X}_n)^2)} \\overset{d}{\\rightarrow} \\mathcal{N}(0,1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{8}\\]\nNote that proving these we are proving \\(\\hat{\\beta}_{1n} \\overset{d}{\\rightarrow} \\mathcal{N}(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{X_n})^2})\\) since\n\\[\\begin{align*}\n\\hat{\\beta}_{1n} &= \\beta_1 + \\frac{\\sum_{i=1}^nW_{ni}}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2} \\\\\n&= \\beta_1 + \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}} \\cdot \\frac{\\sum_{i=1}^{n}W_{ni}}{\\sigma\\sqrt(\\sum_{i=1}^{n}(x_i - \\bar{X})^2)}\\\\\n&\\overset{d}{\\rightarrow} \\beta_1 + \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}} \\cdot Z = \\mathcal{N}\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{X_n})^2}\\right)\n\\end{align*}\\]\nThanks to Lindeberg-Lévy-Feller theorem Theorem 2, we only need to prove that \\(L_1\\) (Equation 2) holds for the sequence \\(\\{W_{ni}\\}_{i=1}^{n}\\) to conclude that Equation 8 holds.\n\n\n\n\n\n\nNote 1: Example Proof\n\n\n\n\n\nThe Lindeberg condition is given by\n\\[\n\\frac{1}{V_n}\\sum_{i=1}^n\n\\mathbb{E}\\!\\left[ W_{ni}^2 \\mathbf{1}\\{ |W_{ni}| &gt; \\varepsilon V_n \\} \\right].\n\\]\nReplacing by the definition of \\(W_{ni}\\),\n\\[\n\\frac{1}{V_n} \\sum_{i=1}^n\n\\mathbb{E}\\!\\left[ ((x_i - \\bar X_n)e_i)^2\n     \\mathbf{1}\\{ |(x_i - \\bar X_n)e_i| \\ge \\varepsilon V_n \\} \\right].\n\\]\nThis can be written as\n\\[\n\\frac{1}{V_n} \\sum_{i=1}^n (x_i - \\bar X_n)^2\n     \\mathbb{E}\\!\\left[ e_i^2 \\mathbf{1}\\left\\{ |e_i| \\ge\n     \\frac{\\varepsilon V_n}{|x_i - \\bar X_n|} \\right\\} \\right].\n\\]\nIt is useful to remove the dependence on \\(i\\) inside the expectation.\nDefine\n\\[\nM := \\max_{j=1,\\dots,n} |x_j - \\bar X_n|.\n\\]\nBy definition, \\(M \\ge |x_i - \\bar X_n|\\), and therefore\n\\[\n\\frac{\\varepsilon V_n}{M} \\le\n\\frac{\\varepsilon V_n}{|x_i - \\bar X_n|}.\n\\]\nThus,\n\\[\n\\mathbf{1}\\!\\left\\{ |e_i| \\ge\n\\frac{\\varepsilon V_n}{|x_i - \\bar X_n|} \\right\\}\n\\le\n\\mathbf{1}\\!\\left\\{ |e_i| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}.\n\\]\nReturning to the condition\nUsing the previous bound,\n\\[\n\\le \\frac{1}{V_n} \\sum_{i=1}^n (x_i - \\bar X_n)^2\n\\mathbb{E}\\!\\left[\ne_i^2 \\mathbf{1}\\!\\left\\{ |e_i| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}\n\\right].\n\\]\nSince the \\(e_i\\) are independent and identically distributed,\n\\[\n= \\frac{1}{V_n} \\left[ \\sum_{i=1}^n (x_i - \\bar X_n)^2 \\right]\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}\n\\right].\n\\]\nUsing the definition of \\(V_n = \\sigma^2 \\sum_{i=1}^n (x_i - \\bar X_n)^2\\),\n\\[\n= \\frac{1}{\\sigma^2}\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge\n\\varepsilon \\frac{\\sqrt{\\sum (x_i - \\bar X_n)^2}}{M}\n\\right\\} \\right].\n\\]\nThus,\n\\[\n= \\frac{1}{\\sigma^2}\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (x_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\right].\n\\]\nIf we impose the condition\n\\[\n\\frac{M^2}{\\sum_{i=1}^n (x_i - \\bar X_n)^2}\n=\n\\frac{\\max_{j}|x_j - \\bar X_n|^2}\n{\\sum_{i=1}^n (x_i - \\bar X_n)^2}\n\\longrightarrow 0 \\qquad (n \\to \\infty),\n\\]\nthen\n\\[\ne_1^2 \\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (x_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\longrightarrow 0\n\\quad \\text{a.s.}\n\\]\n\nLebesgue Dominated Convergence Theorem\nSuppose \\(|X_n| \\le Y\\) for all \\(n\\), with \\(\\mathbb{E}[Y] &lt; \\infty\\), and \\(X_n \\to X\\) almost surely as \\(n \\to \\infty\\). Then\n\\[\n\\mathbb{E}[X_n] \\to \\mathbb{E}[X].\n\\]\n\nApplying this to our setting, we have\n\\[\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (x_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\longrightarrow 0,\n\\]\nand uniformly,\n\\[\n0 \\le\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (x_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\le e_1^2.\n\\]\nSince \\(\\mathbb{E}[e_1^2] = \\sigma^2 &lt; \\infty\\), by the Dominated Convergence Theorem,\n\\[\n\\mathbb{E}\\!\\left[\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (x_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\right]\n\\longrightarrow 0 \\qquad (n \\to \\infty).\n\\]\n\n\n\nIn Note 1, we conclude that we can change the identical distribution requirement for\n\\[\n\\max_{1 \\leq j \\leq n} \\frac{(x_j - \\bar{X}_n)^2}{\\sum_{i=1}^{n}(x_i - \\bar{X}_n)^2} \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\n\\]\nThat is a much relaxed requirement.\n\nExample Simulation\nTo simulate this experiment, we generate data from the linear model \\(y = 1 + 2x + \\epsilon\\), where the errors \\(\\epsilon\\) follow an exponential distribution with variance \\(\\sigma^{2} = 64\\). Using Monte Carlo with 5000 replications, we examine the sampling distribution of the OLS estimator for different sample sizes. As shown in the widget, increasing the sample size causes the sampling distribution to approach a normal distribution centered around the true value \\(\\beta_{1} = 2\\).\n\ndata = aq.loadCSV(\"https://raw.githubusercontent.com/brunotancredi/lindeberg-feller/refs/heads/main/ols_montecarlo_simulation.csv\")\n\nviewof z = Inputs.range(\n  [2, 500], \n  {value: 2, step: 1, label: \"Sample Size:\"}\n)\n\nols_data = data.filter(aq.escape(d =&gt; d.n === z))\n\nbinThresholds = {\n  const nbins = 30; // &lt;- your desired number of bins\n  const [xmin, xmax] = d3.extent(ols_data, d =&gt; d.ols_estimator);\n  const step = (xmax - xmin) / nbins;\n  // nbins bins = nbins+1 edges\n  return d3.range(xmin, xmax + step * 1.0001, step);\n}\n\nlength = ols_data.numRows()\n\nPlot.plot({\n  width: 1000,\n  height: 500,\n  y: {grid: true},\n  x: {round: true},\n  marks: [\n    Plot.rectY(\n      ols_data,\n      Plot.binX(\n        { y: \"proportion\" },\n        {\n          x: \"ols_estimator\",\n          thresholds: binThresholds,   // fixed # of bins\n          inset: 0\n        }\n      )\n    ),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "index.html#lyapounovs-condition",
    "href": "index.html#lyapounovs-condition",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Lyapounov’s Condition",
    "text": "Lyapounov’s Condition\nSometimes proving the Lindeberg’s condition (Equation 2) can be difficult, so we can use an alternative condition known as Lyapounov’s condition. So we can start by checking if \\(L_y\\) is true and if it fails, we can try to prove that \\(L_1\\) is true.\nLet \\(\\{X_n\\}_{n \\geq 1}\\) a sequence of random variables given as before and assume, in addition, that \\(\\mathbb{E}[|X_k - \\mu_k|^{2+\\delta}] &lt; \\infty\\) for all \\(k\\) and some \\(\\delta &gt; 0\\). The advantage of\n\\[\nL_y(n, r) = \\frac{1}{V_n^{2+\\delta}}\\sum^{n}_{k=1}\\mathbb{E}|X_k - \\mu_k|^{2+\\delta} \\rightarrow 0 \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{9}\\]\n\\[\n\\Rightarrow \\frac{1}{V_n}\\sum^n_{k=1}(X_k - \\mu_k) \\overset{d}{\\rightarrow} \\mathcal{N}(0,1) \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\]\nFor proving this condition we can show that when the Lyapunov’s condition (Equation 9) is satisfied so is the Lindeberg’s condition (Equation 2); i.e \\(L_y \\Rightarrow L_1\\).\nIt’s worth noticing that if, \\(L_1\\) holds doesn’t implies that \\(L_y\\) holds too."
  },
  {
    "objectID": "index.html#comments-when-it-fails",
    "href": "index.html#comments-when-it-fails",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Comments: When it fails?",
    "text": "Comments: When it fails?\nThe advantage of the Lindeberg–Lévy–Feller theorem is that it relaxes the strong assumption of identical distribution required in the classical Central Limit Theorem, replacing it with the more general Lindeberg condition. It is important to note that independence among random variables is still required. Moreover, as discussed earlier, the Lindeberg condition essentially limits the presence of heavy-tailed distributions. However, as shown by (Newman, 2004), we can find heavy-tailed behavior everywhere. In his article, Newman lists several examples, including city populations, individual wealth, citations of scientific papers, website traffic, and others.\nAs an illustration, consider the empirical example using the dataset (Ache, 2022), which contains 4,894,081 scientific papers and their corresponding citation counts.\nAs we can see in Figure 2, the sampling distribution of the sample means doesn’t seem to follow a normal distribution; a clear heavy tail is present even when we increase the sample size. The probability of seeing an observation farther than 5 standard deviations under a normal distribution is about once in 3.5 million observations, and farther than 10 is about once in one hundred sextillion but we have a lot of them!\nSo we have two possibilities: either I ran the experiment infinitely many times, changing the seed until I got this crazy result, or the convergence to the normal distribution is simply not happening. I would ask you to believe my word that I didn’t mislead the experiment.\n\n\n\n\n\n\nFigure 2: Centered sampling distributions of the sample means. Observations exceeding 5 standard deviations are highlighted in green, while those exceeding 10 standard deviations are highlighted in red.\n\n\n\nIn Figure 3, we can see how \\(L_1\\) is not satisficied, since instead of converging to 0 it bounces, even with \\(n=50000\\).\n\n\n\n\n\n\nFigure 3: Evolution of the empirical Lindeberg condition for an arbitrary ε = 0.2\n\n\n\nAs a final comment, here we are working with citations in papers that have a certain limitation (the most cited papers here have around 50,000 citations). Now think about views of a webpage: how many views does www.youtube.com have compared to this humble page that talks about the Lindeberg–Lévy–Feller theorem? The variance there will be much higher, and this is just another example. So be careful, a heavy-tailed distribution might be waiting for you around the corner."
  },
  {
    "objectID": "index.html#comic-relief",
    "href": "index.html#comic-relief",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Comic Relief",
    "text": "Comic Relief\n\nThe Swedish mathematician Harald Cramér met Lindeberg in 1922. He later recalled this story about Lindeberg and the beautiful farm he owned. “When he was reproached for not being sufficiently active in his scientific work, he said ‘Well, I am really a farmer.’ And if somebody happened to say that his farm was not properly cultivated, his answer was ‘Of course my real job is to be a professor.’ …” (Wikipedia contributors, 2025)\n\n\nReferences\n\n\n\nAche, M. (2022). Citation network dataset. https://www.kaggle.com/datasets/mathurinache/citation-network-dataset.\n\n\nGut, A. (2013). Probability: A graduate course. Springer New York. https://books.google.ca/books?id=9TmRgPg-6vgC\n\n\nLi, J. J. (2020). Large sample theory (fall 2020). YouTube playlist. https://www.youtube.com/playlist?list=PLAYxx7zX5F1NKukTVwMADi1D5dbufWJkz\n\n\nNewman, M. (2004). Power laws, pareto distributions and zipf’s law. Contemporary Physics - CONTEMP PHYS, 46. https://doi.org/10.1080/00107510500052444\n\n\nVaart, A. W. van der. (1998). Asymptotic statistics. Cambridge University Press.\n\n\nWikipedia contributors. (2025). Jarl waldemar lindeberg. https://en.wikipedia.org/wiki/Jarl_Waldemar_Lindeberg."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "",
    "text": "Here, I’m going to present generalizations of the Central Limit Theorem, such as the Lindeberg–Levy–Feller Central Limit Theorem and Lyapunov’s Central Limit Theorem. The applications of the asymptotic behavior of the sample mean of random variables extend across all areas of statistics, being used for inference, hypothesis testing, estimation theory, among many others.\nThe reference material has been drawn from the books Probability: A Graduate Course (Gut, 2013) and Asymptotic Statistics (Vaart, 1998), from the Large Sample Theory lectures by Prof. Jessica Li available on YouTube (Li, 2020), and from the lecture notes by Prof. Marco Scavino for the Probabilidad II course at the Universidad de la República."
  },
  {
    "objectID": "presentation.html#central-limit-theorem",
    "href": "presentation.html#central-limit-theorem",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nProbably we are familiar with the Central Limit Theorem (Theorem 1).\n\nTheorem 1 (Central Limit Theorem) Let \\(\\{X_n\\}_{n \\geq  1}\\) be a sequence of independent and identical distributed random variables with finite expectation \\(\\mu\\) and and positive, finite variance \\(\\sigma^2\\), and set \\(S_n = X_1 + X_2 + \\dots + X_n, n \\geq 1\\). Then\n\\[\n\\frac{S_n - n\\mu}{\\sigma\\sqrt{n}} \\, \\overset{d}{\\rightarrow} \\, \\mathcal{N}(0,1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{1}\\]\n\n\nSimulating the CLT\n\nimport { aq, op } from '@uwdata/arquero'\n\nmy_data = aq.loadCSV(\"https://raw.githubusercontent.com/brunotancredi/lindeberg-feller/refs/heads/main/CLT_simulation_data.csv\")\n\nviewof n = Inputs.range(\n  [1, 400], \n  {value: 1, step: 1, label: \"Sample Size:\"}\n)\n\nviewof distribution = Inputs.radio([\"Uniform\", \"Exponential\"], {value: \"Uniform\", label: \"Distribution\"})\n\nnormalData = Array.from({ length: 100 }, (_, i) =&gt; {\n  let x = -3 + (i / 99) * 6; // Range from -3 to 3\n  return { x, y: Math.exp(-0.5 * x * x) / Math.sqrt(2 * Math.PI) };\n});\n\n\nmy_data_2 = my_data.params({ distribution }).filter(d =&gt; d.distribution === distribution);\n\npoint = Array.from({length: 1}, (_, i) =&gt; {return {x: 0, y: 0.5}})\n\nPlot.plot({\n    width: 1000,\n    height: 500,\n    y: {grid: true},\n    x: {round: true},    \n    marks: [\n      Plot.rectY(my_data_2, \n                Plot.binX({y: (a, bin) =&gt; {\n                              return a.length / (5000*(bin.x2 - bin.x1));\n                           }}, \n                          {x: {value: n.toString(), \n                               inset: 0,\n                               thresholds: d3.range(-3,3 + 0.11, 0.11),\n                               domain: [-3, 3],\n                               }})),\n      Plot.ruleY([0]),\n      Plot.line(normalData, { x: \"x\", y: \"y\", stroke: \"red\" }), //Normal\n      Plot.dot(point, {x: \"x\", y: \"y\", r: 0})    \n    ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe problem with the theorem is that requires a two strong assumptions idependency and identical distribution, what if I told you that we can relax one of the conditions?"
  },
  {
    "objectID": "presentation.html#lindeberg-lévy-feller",
    "href": "presentation.html#lindeberg-lévy-feller",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Lindeberg-Lévy-Feller",
    "text": "Lindeberg-Lévy-Feller\nLet \\(\\{X_n\\}_{n \\geq 1}\\) be a sequence of independent random variables with finite variances, and set, for \\(k \\geq 1, \\mathbb{E}[X_k]= \\mu_k, Var[X_k] = \\sigma_k^2\\) and for \\(n \\geq 1, S_n = \\sum^{n}_{k=1}X_k\\) and \\(V_n^2 = \\sum^n_{k=1}\\sigma_k^2\\)\n\nConditions\n\\[\nL_1(n) = \\frac{1}{V_n^2}\\sum^{n}_{k=1}\\mathbb{E}\\big[(X_k - \\mu_k)^2\\mathbf{1}_{\\{|X_k - \\mu_k| \\gt \\epsilon V_n\\}}\\big] \\underset{n \\rightarrow \\infty}{\\rightarrow} 0, \\quad \\forall \\epsilon \\gt 0\n\\tag{2}\\]\n\\[\nL_2(n) = \\max_{1 \\leq k \\leq n} \\frac{\\sigma^2_k}{V_n} \\underset{n \\rightarrow \\infty}{\\rightarrow} 0,\n\\tag{3}\\]\n\n\nInterpretating the conditions\n\\(L_1\\) (Lindeberg’s Condition): This condition requires that the proportion of variance represented by values far from the mean is negligible. In other words, what it asks us is that the original distributions of \\(X_k\\) do not have heavy tails.\n\n\n\n\n\n\nFigure 1: Rudimentary visualization of Lindeberg Condition\n\n\n\n\\(L_2\\): To understand condition \\(L_2\\), it is useful to consider when it fails. The condition is violated when \\(\\sigma_k^2\\) represents a non-negligible portion of the total sum of variances. In other words, this occurs when the variance of a single term contributes significantly to the total variance. For example, this could happen when \\(\\sigma^2_k = \\sum_{i=1}^{k-1} \\sigma_i^2\\). So we are asking o each \\(X_k\\) to have a limited variance.\n\n\nTheorem\nUsing the conditions defined above we can state the theorem as follows\n\nTheorem 2 (Lindeberg-Lévy-Feller Theorem) \n(i). If \\(L_1\\) (Equation 2) is satisfied , then so is \\(L_2\\) (Equation 3) and\n\\[\n\\frac{1}{V_n}\\sum^{n}_{k=1}(X_k - \\mu_k) \\overset{d}{\\rightarrow} \\mathcal{N}(0, 1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{4}\\]\n(ii). If Equation 3 and Equation 4 are satisfied, then so is Equation 2\n\nAs we can see, we no longer require that the sequence be identically distributed to obtain a normal distribution. We are only requiring (i) that \\(L_1\\) is satisfied."
  },
  {
    "objectID": "presentation.html#what-do-we-need-to-prove-it",
    "href": "presentation.html#what-do-we-need-to-prove-it",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "What do we need to prove it?",
    "text": "What do we need to prove it?\nThe proof of the theorem is extensive, so here I’m going to present a sketch of the proof.\nFirst, for simplicity and without loss of generality, we assume that \\(\\mu_k = 0\\) for all \\(k \\geq 1\\).\nSo we want to prove that\n\\[\n\\frac{S_n}{V_n} \\overset{d}{\\rightarrow} Z,\n\\quad \\text{as} \\quad n \\rightarrow \\infty\n\\]\nwhere \\(Z \\sim \\mathcal{N}(0,1)\\).\n\nFor the proof we are going to use the Lévy’s Continuity Theorem (Theorem 3), which establishes a relationship between convergence in distribution and convergence of characteristic functions.\n\nTheorem 3 (Lévy’s Continuity Theorem) Let \\(\\{X_n\\}_{n \\geq 1}\\) be a sequence of random variables. Then\n\\[\n\\begin{aligned}\n&\\qquad \\varphi_{X_n}(t) \\rightarrow \\varphi_X(t)\n\\quad \\text{as } n \\rightarrow \\infty\n\\quad \\forall t \\in \\mathbb{R}, \\\\\n&\\Leftrightarrow \\quad\nX_n \\overset{d}{\\rightarrow} X\n\\quad \\text{as } n \\rightarrow \\infty.\n\\end{aligned}\n\\]\n\nBy proving that \\(\\varphi_{\\frac{S_n}{V_n}}(t) \\to \\varphi_Z(t)\\) we can conclude that \\(\\frac{S_n}{V_n} \\xrightarrow{d} Z\\) which is the result we want.\nTo show that \\(\\varphi_{\\frac{S_n}{V_n}}(t)\\) converges pointwise to the characteristic function of the standard normal distribution \\(\\varphi_Z(t)\\), we proceed as follows: We approximate \\(\\varphi_{\\frac{S_n}{V_n}}(t)\\) using its Taylor expansion. This approximation leads to an expression that coincides with \\(\\varphi_Z(t)\\), and we then show that the remainder (error) term tends to zero as \\(n \\to \\infty\\).\n\n\n\n\n\n\nSketch of the proof\n\n\n\n\n\n\\[\n\\varphi_{\\frac{S_n}{V_n}}(t) = \\varphi_{S_n}\\!\\left( \\frac{t}{V_n} \\right)\n\\]\n\\[\n\\left( S_n = \\sum_{k=1}^n X_k, \\quad X_k \\text{ indep.} \\right)\n\\]\n\\[\n= \\prod_{k=1}^n \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right)\n= \\left( \\varphi = \\exp(\\log \\varphi) \\right)\n\\]\n\\[\n= \\exp\\!\\left\\{ \\sum_{k=1}^n \\log \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right) \\right\\}\n\\]\n\\[\n\\overset{(*)}{\\approx}\n\\exp\\!\\left\\{ -\\sum_{k=1}^n \\left( 1 - \\varphi_{X_k}\\!\\left( \\frac{t}{V_n} \\right) \\right) \\right\\}\n\\]\n\\[\n\\overset{(**)}{\\approx}\n\\exp\\!\\left\\{ -\\sum_{k=1}^n\n\\left( 1 - \\left( 1 + \\frac{it}{V_n} \\mathbb{E}X_k + \\frac{(it)^2}{2V_n^2} \\mathbb{E}X_k^2 \\right) \\right)\n\\right\\}\n\\]\n\\[\n= \\exp\\!\\left\\{ -\\sum_{k=1}^n\n\\left( 1 - \\left( 1 - \\frac{t^2}{2V_n^2}\\sigma_k^2 \\right) \\right)\n\\right\\}\n\\]\n\\[\n= \\exp\\!\\left\\{ -\\frac{t^2}{2V_n^2} \\sum_{k=1}^n \\sigma_k^2 \\right\\}\n= e^{-\\frac{t^2}{2}}.\n\\]\nWhere \\((*)\\) follows from the Taylor expansion of \\(\\log(1 + x)\\) around \\(x = 0\\) and \\((**)\\) follows from the Taylor expansion of \\(\\varphi_{X_k}(t)\\) around \\(t = 0\\). We need to prove that those \\(\\approx\\) are \\(=\\) when \\(n \\to \\infty\\), i.e. the error terms vanish.\n\n\n\nThis combination of Lévy’s Continuity Theorem and Taylor series expansion can be used to prove different theorems, including the original Central Limit Theorem."
  },
  {
    "objectID": "presentation.html#example",
    "href": "presentation.html#example",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Example",
    "text": "Example\nA cool application of Lindeberg-Feller theorem is in the area of Linear Regression. For those that are in a Regression course, or already take one, you will have seen that the Ordinary Least Squares (OLS) estimator follows a normal distribution when the errors are normal. But … what if the errors are not normal? Well, under certain assumptions we can use the Lindeberg-Lévy-Feller theorem to show that the OLS estimator assympotically follows a normal distribution. For simplicity we are going to prove it for the estimator of the slope coefficient in a simple linear regression model (SLR).\n\\[\ny_i = \\beta_0 + \\beta_1x_i + e_i, \\quad e_i \\quad i.i.d \\quad \\forall i = 1, \\dots, n\n\\tag{5}\\]\nA simple linear regression model is given by Equation 5, where \\(\\mathbb{E}[e_i] = 0\\) and \\(\\text{Var}(e_i) = \\sigma^2\\). We have that the estimator for the slope in SLR is given by\n\\[\n\\hat{\\beta}_{1n} = \\frac{\\sum_{i=1}^n(y_i - \\bar{Y}_ n)(x_i - \\bar{X}_n)}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\tag{6}\\]\nYou can notice that in Equation 6 we have a subindex \\(n\\), this is because \\(\\hat{\\beta}_{1n}\\) (and the means \\(\\bar{X}_n\\), \\(\\bar{Y}_n\\) too) changes with the sample size, and we want to study its asymptotic behavior when \\(n \\to \\infty\\).\nAfter some math we can rewrite \\(\\hat{\\beta}_{1n}\\) as a linear combination of the erros\n\\[\n\\hat{\\beta}_{1n} = \\beta_1 + \\frac{\\sum_{i=1}^n(x_i - \\bar{X}_n)e_i}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\tag{7}\\]\nWe can define the sequence of random variables\n\\[\nW_{ni} := (x_i - \\bar{X}_n)e_i\n\\]\nAnd rewrite Equation 7 as\n\\[\n\\hat{\\beta}_{1n} = \\beta_1 + \\frac{\\sum_{i=1}^nW_{ni}}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}\n\\]\nWe have that the expected value of \\(W_{ni}\\) is zero and its variance is given by \\(\\sigma^2(x_i - \\bar{X})^2\\), since the only random part is \\(e_i\\) and they are i.i.d, with mean zero and variance \\(\\sigma^2\\).\nOur goal is to proof that \\[\n\\frac{\\sum_{i=1}^{n}W_{ni}}{\\sigma\\sqrt(\\sum_{i=1}^{n}(x_i - \\bar{X}_n)^2)} \\overset{d}{\\rightarrow} \\mathcal{N}(0,1), \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{8}\\]\nNote that proving these we are proving \\(\\hat{\\beta}_{1n} \\overset{d}{\\rightarrow} \\mathcal{N}(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{X_i})^2})\\) since\n\\[\\begin{align*}\n\\hat{\\beta}_{1n} &= \\beta_1 + \\frac{\\sum_{i=1}^nW_{ni}}{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2} \\\\\n&= \\beta_1 + \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}} \\cdot \\frac{\\sum_{i=1}^{n}W_{ni}}{\\sigma\\sqrt(\\sum_{i=1}^{n}(x_i - \\bar{X})^2)}\\\\\n&\\overset{d}{\\rightarrow} \\beta_1 + \\frac{\\sigma}{\\sqrt{\\sum_{i=1}^n(x_i - \\bar{X}_n)^2}} \\cdot Z = \\mathcal{N}\\left(\\beta_1, \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i - \\bar{X_i})^2}\\right)\n\\end{align*}\\]\nThanks to Lindeberg-Lévy-Feller theorem Theorem 2, we only need to prove that \\(L_1\\) (Equation 2) holds for the sequence \\(\\{W_{ni}\\}_{i=1}^{n}\\) to conclude that Equation 8 holds.\n\ndata = aq.loadCSV(\"https://raw.githubusercontent.com/brunotancredi/lindeberg-feller/refs/heads/main/ols_montecarlo_simulation.csv\")\n\nviewof z = Inputs.range(\n  [2, 500], \n  {value: 2, step: 1, label: \"Sample Size:\"}\n)\n\nols_data = data.filter(aq.escape(d =&gt; d.n === z))\n\nbinThresholds = {\n  const nbins = 30; // &lt;- your desired number of bins\n  const [xmin, xmax] = d3.extent(ols_data, d =&gt; d.ols_estimator);\n  const step = (xmax - xmin) / nbins;\n  // nbins bins = nbins+1 edges\n  return d3.range(xmin, xmax + step * 1.0001, step);\n}\n\nlength = ols_data.numRows()\n\nPlot.plot({\n  width: 1000,\n  height: 500,\n  y: {grid: true},\n  x: {round: true},\n  marks: [\n    Plot.rectY(\n      ols_data,\n      Plot.binX(\n        { y: \"proportion\" },\n        {\n          x: \"ols_estimator\",\n          thresholds: binThresholds,   // fixed # of bins\n          inset: 0\n        }\n      )\n    ),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 1: Example Proof\n\n\n\n\n\nThe Lindeberg condition is given by\n\\[\n\\frac{1}{V_n}\\sum_{i=1}^n\n\\mathbb{E}\\!\\left[ W_{ni}^2 \\mathbf{1}\\{ |W_{ni}| &gt; \\varepsilon V_n \\} \\right].\n\\]\nReplacing by the definition of \\(W_{ni}\\),\n\\[\n\\frac{1}{V_n} \\sum_{i=1}^n\n\\mathbb{E}\\!\\left[ ((X_i - \\bar X_n)e_i)^2\n     \\mathbf{1}\\{ |(X_i - \\bar X_n)e_i| \\ge \\varepsilon V_n \\} \\right].\n\\]\nThis can be written as\n\\[\n\\frac{1}{V_n} \\sum_{i=1}^n (X_i - \\bar X_n)^2\n     \\mathbb{E}\\!\\left[ e_i^2 \\mathbf{1}\\left\\{ |e_i| \\ge\n     \\frac{\\varepsilon V_n}{|X_i - \\bar X_n|} \\right\\} \\right].\n\\]\nIt is useful to remove the dependence on \\(i\\) inside the expectation.\nDefine\n\\[\nM := \\max_{j=1,\\dots,n} |X_j - \\bar X_n|.\n\\]\nBy definition, \\(M \\ge |X_i - \\bar X_n|\\), and therefore\n\\[\n\\frac{\\varepsilon V_n}{M} \\le\n\\frac{\\varepsilon V_n}{|X_i - \\bar X_n|}.\n\\]\nThus,\n\\[\n\\mathbf{1}\\!\\left\\{ |e_i| \\ge\n\\frac{\\varepsilon V_n}{|X_i - \\bar X_n|} \\right\\}\n\\le\n\\mathbf{1}\\!\\left\\{ |e_i| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}.\n\\]\nReturning to the condition\nUsing the previous bound,\n\\[\n\\le \\frac{1}{V_n} \\sum_{i=1}^n (X_i - \\bar X_n)^2\n\\mathbb{E}\\!\\left[\ne_i^2 \\mathbf{1}\\!\\left\\{ |e_i| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}\n\\right].\n\\]\nSince the \\(e_i\\) are independent and identically distributed,\n\\[\n= \\frac{1}{V_n} \\left[ \\sum_{i=1}^n (X_i - \\bar X_n)^2 \\right]\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge \\frac{\\varepsilon V_n}{M} \\right\\}\n\\right].\n\\]\nUsing the definition of \\(V_n = \\sigma^2 \\sum_{i=1}^n (X_i - \\bar X_n)^2\\),\n\\[\n= \\frac{1}{\\sigma^2}\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge\n\\varepsilon \\frac{\\sqrt{\\sum (X_i - \\bar X_n)^2}}{M}\n\\right\\} \\right].\n\\]\nThus,\n\\[\n= \\frac{1}{\\sigma^2}\n\\mathbb{E}\\!\\left[\ne_1^2 \\mathbf{1}\\!\\left\\{ |e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (X_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\right].\n\\]\nIf we impose the condition\n\\[\n\\frac{M^2}{\\sum_{i=1}^n (X_i - \\bar X_n)^2}\n=\n\\frac{\\max_{j}|X_j - \\bar X_n|^2}\n{\\sum_{i=1}^n (X_i - \\bar X_n)^2}\n\\longrightarrow 0 \\qquad (n \\to \\infty),\n\\]\nthen\n\\[\ne_1^2 \\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (X_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\longrightarrow 0\n\\quad \\text{a.s.}\n\\]\nUsing the Lebesgue Dominated Convergence Theorem\nSuppose \\(|X_n| \\le Y\\) for all \\(n\\), with \\(\\mathbb{E}[Y] &lt; \\infty\\), and\n\\(X_n \\to X\\) almost surely as \\(n \\to \\infty\\). Then\n\\[\n\\mathbb{E}[X_n] \\to \\mathbb{E}[X].\n\\]\nApplying this to our setting, we have\n\\[\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (X_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\longrightarrow 0,\n\\]\nand uniformly,\n\\[\n0 \\le\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (X_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\le e_1^2.\n\\]\nSince \\(\\mathbb{E}[e_1^2] = \\sigma^2 &lt; \\infty\\), by the Dominated Convergence Theorem,\n\\[\n\\mathbb{E}\\!\\left[\ne_1^2\\,\\mathbf{1}\\!\\left\\{\n|e_1| \\ge\n\\varepsilon \\sqrt{\\frac{\\sum (X_i - \\bar X_n)^2}{M^2}}\n\\right\\}\n\\right]\n\\longrightarrow 0 \\qquad (n \\to \\infty).\n\\]\n\n\n\nIn Note 1, we conclude that we can change the identical distribution requirement for\n\\[\n\\max_{1 \\leq j \\leq n} \\frac{(x_j - \\bar{X}_n)^2}{\\sum_{i=1}^{n}(x_i - \\bar{X}_n)^2} \\underset{n \\rightarrow \\infty}{\\rightarrow} 0\n\\]\nThat is a much relaxed requirement.\n\nExample Simulation\nTo simulate this experiment, we generate data from the linear model \\(y = 1 + 2x + \\epsilon\\), where the errors \\(\\epsilon\\) follow an exponential distribution with variance \\(\\sigma^{2} = 64\\). Using Monte Carlo with 5000 replications, we examine the sampling distribution of the OLS estimator for different sample sizes. As shown in the widget, increasing the sample size causes the sampling distribution to approach a normal distribution centered around the true value \\(\\beta_{1} = 2\\)."
  },
  {
    "objectID": "presentation.html#lyapounovs-condition",
    "href": "presentation.html#lyapounovs-condition",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Lyapounov’s Condition",
    "text": "Lyapounov’s Condition\nSometimes proving the Lindeberg’s condition (Equation 2) can be difficult, so we can use an alternative condition known as Lyapounov’s condition. So we can start by checking if \\(L_y\\) is true and if it fails, we can try to prove that \\(L_1\\) is true.\nLet \\(\\{X_n\\}_{n \\geq 1}\\) a sequence of random variables given as before and assume, in addition, that \\(\\mathbb{E}[|X_k - \\mu_k|^{2+\\delta}] &lt; \\infty\\) for all \\(k\\) and some \\(\\delta &gt; 0\\). The advantage of\n\\[\nL_y(n, r) = \\frac{1}{V_n^{2+\\delta}}\\sum^{n}_{k=1}\\mathbb{E}|X_k - \\mu_k|^{2+\\delta} \\rightarrow 0 \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\tag{9}\\]\n\\[\n\\Rightarrow \\frac{1}{V_n}\\sum^n_{k=1}(X_k - \\mu_k) \\overset{d}{\\rightarrow} \\mathcal{N}(0,1) \\quad \\text{as} \\quad n \\rightarrow \\infty\n\\]\nFor proving this condition we can show that when the Lyapunov’s condition (Equation 9) is satisfied so is the Lindeberg’s condition (Equation 2); i.e \\(L_y \\Rightarrow L_1\\).\nIt’s worth noticing that if, \\(L_1\\) holds doesn’t implies that \\(L_y\\) holds too."
  },
  {
    "objectID": "presentation.html#comments-when-it-fails",
    "href": "presentation.html#comments-when-it-fails",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Comments: When it fails?",
    "text": "Comments: When it fails?\nThe advantage of the Lindeberg–Lévy–Feller theorem is that it relaxes the strong assumption of identical distribution required in the classical Central Limit Theorem, replacing it with the more general Lindeberg condition. It is important to note that independence among random variables is still required. Moreover, as discussed earlier, the Lindeberg condition essentially limits the presence of heavy-tailed distributions. However, as shown by (Newman, 2004), we can find heavy-tailed behavior everywhere. In his article, Newman lists several examples, including city populations, individual wealth, citations of scientific papers, website traffic, and others.\nAs an illustration, consider the empirical example using the dataset (Ache, 2022), which contains 4,894,081 scientific papers and their corresponding citation counts.\nAs we can see in Figure 2, the sampling distribution of the sample means doesn’t seem to follow a normal distribution; a clear heavy tail is present even when we increase the sample size. The probability of seeing an observation farther than 5 standard deviations under a normal distribution is …, and farther than 10 is …, and we have a lot of them!\nSo we have two possibilities: either I ran the experiment infinitely many times, changing the seed until I got this crazy result, or the convergence to the normal distribution is simply not happening. I would ask you to believe my word that I didn’t mislead the experiment.\n\n\n\n\n\n\nFigure 2: Centered sampling distributions of the sample means. Observations exceeding 5 standard deviations are highlighted in green, while those exceeding 10 standard deviations are highlighted in red.\n\n\n\nIn Figure 3, we can see how \\(L_1\\) is not satisficied, since instead of converging to 0 it bounces, even with \\(n=50000\\).\n\n\n\n\n\n\nFigure 3: Evolution of the empirical Lindeberg condition for an arbitrary ε = 0.2\n\n\n\nAs a final comment, here we are working with citations in papers that have a certain limitation (the most cited papers here have around 50,000 citations). Now think about views of a webpage: how many views does www.youtube.com have compared to this humble page that talks about the Lindeberg–Lévy–Feller theorem? The variance there will be much higher, and this is just another example. So be careful, a heavy-tailed distribution might be waiting for you around the corner."
  },
  {
    "objectID": "presentation.html#comic-relief",
    "href": "presentation.html#comic-relief",
    "title": "Lindeberg-Lévy-Feller Theorem",
    "section": "Comic Relief",
    "text": "Comic Relief\n\nThe Swedish mathematician Harald Cramér met Lindeberg in 1922. He later recalled this story about Lindeberg and the beautiful farm he owned. “When he was reproached for not being sufficiently active in his scientific work, he said ‘Well, I am really a farmer.’ And if somebody happened to say that his farm was not properly cultivated, his answer was ‘Of course my real job is to be a professor.’ …” (Wikipedia contributors, 2025)\n\n\nReferences\n\n\n\nAche, M. (2022). Citation network dataset. https://www.kaggle.com/datasets/mathurinache/citation-network-dataset.\n\n\nGut, A. (2013). Probability: A graduate course. Springer New York. https://books.google.ca/books?id=9TmRgPg-6vgC\n\n\nLi, J. J. (2020). Large sample theory (fall 2020). YouTube playlist. https://www.youtube.com/playlist?list=PLAYxx7zX5F1NKukTVwMADi1D5dbufWJkz\n\n\nNewman, M. (2004). Power laws, pareto distributions and zipf’s law. Contemporary Physics - CONTEMP PHYS, 46. https://doi.org/10.1080/00107510500052444\n\n\nVaart, A. W. van der. (1998). Asymptotic statistics. Cambridge University Press.\n\n\nWikipedia contributors. (2025). Jarl waldemar lindeberg. https://en.wikipedia.org/wiki/Jarl_Waldemar_Lindeberg."
  }
]